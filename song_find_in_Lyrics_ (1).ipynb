{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#word_index\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define the texts\n",
        "texts = ['I like girls, I love parrots', 'I love humans being!']\n",
        "\n",
        "# Create a Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer to the texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert the texts to word indices\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert the texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "print(\"Word Index:\")\n",
        "print(word_index)\n",
        "print(\"\\nSequences:\")\n",
        "print(sequences)\n",
        "\n",
        "\n",
        "#This code defines two texts, creates a Tokenizer, fits it to the texts, converts the texts to word indices, and then converts the texts to sequences. The word index and sequences are then printed out."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZJobr5yOMZP",
        "outputId": "13043fb5-349e-4dad-e52c-1c26f685bd60"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:\n",
            "{'i': 1, 'love': 2, 'like': 3, 'girls': 4, 'parrots': 5, 'humans': 6, 'being': 7}\n",
            "\n",
            "Sequences:\n",
            "[[1, 3, 4, 1, 2, 5], [1, 2, 6, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pad_sequences\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the texts\n",
        "texts = ['I like dog', 'I love cat', 'I love human being']\n",
        "test_texts = ['I like dog', 'I don\\'t like cat', 'I like parents', 'I don\\'t like dog, eats & parrots']\n",
        "\n",
        "# Create a Tokenizer\n",
        "tokenizer = Tokenizer(num_words=100, oov_token='[UNK]')\n",
        "\n",
        "# Fit the tokenizer to the texts\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert the texts to word indices\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert the texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Convert the test texts to sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad the sequences\n",
        "padded_sequences = pad_sequences(sequences, padding='pre', maxlen=20, truncating='post')\n",
        "padded_test_sequences = pad_sequences(test_sequences, padding='pre', maxlen=20, truncating='post')\n",
        "\n",
        "print(\"Word Index:\")\n",
        "print(word_index)\n",
        "print(\"\\nSequences:\")\n",
        "print(sequences)\n",
        "print(\"\\nTest Sequences:\")\n",
        "print(test_sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded_sequences)\n",
        "print(\"\\nPadded Test Sequences:\")\n",
        "print(padded_test_sequences)\n",
        "\n",
        "\n",
        "#This code defines two sets of texts, creates a Tokenizer, fits it to the first set of texts, converts the texts to word indices, converts the texts to sequences, and then pads the sequences to a maximum length of 20. The word index, sequences, test sequences, padded sequences, and padded test sequences are then printed out."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC-eCyT9OMLY",
        "outputId": "e66f830d-f0ca-445a-9931-b8acee82739c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:\n",
            "{'[UNK]': 1, 'i': 2, 'love': 3, 'like': 4, 'dog': 5, 'cat': 6, 'human': 7, 'being': 8}\n",
            "\n",
            "Sequences:\n",
            "[[2, 4, 5], [2, 3, 6], [2, 3, 7, 8]]\n",
            "\n",
            "Test Sequences:\n",
            "[[2, 4, 5], [2, 1, 4, 6], [2, 4, 1], [2, 1, 4, 5, 1, 1]]\n",
            "\n",
            "Padded Sequences:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 5]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 6]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 7 8]]\n",
            "\n",
            "Padded Test Sequences:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 5]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 4 6]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 4 5 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#part of speech tagging\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Create a Porter stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Create a WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Test the stemmer and lemmatizer\n",
        "print(ps.stem(\"playing\"))  # Output: play\n",
        "print(lemmatizer.lemmatize(\"playing\", pos='v'))  # Output: play\n",
        "print(lemmatizer.lemmatize(\"fairly\", pos='a'))  # Output: fairly\n",
        "print(lemmatizer.lemmatize(\"letter\", pos='n'))  # Output: letter\n",
        "\n",
        "# Part-of-speech tagging\n",
        "text = \"I am playing football\"\n",
        "tagged_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "print(tagged_text)\n",
        "\n",
        "\n",
        "#This code uses NLTK to perform stemming and lemmatization on words, and also demonstrates part-of-speech tagging. Note that lemmatization requires a part-of-speech tag to produce the correct result."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKlmVgNIVSfI",
        "outputId": "f5851d92-197d-4430-ecb6-9f4073d85d8f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "play\n",
            "fairly\n",
            "letter\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('playing', 'VBG'), ('football', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VcAyRKOat7iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe142361-b485-4d25-8c26-d8d5522a8596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from tensorflow.keras.layers import Bidirectional,Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CxFyZ-sWbTJW"
      },
      "outputs": [],
      "source": [
        "a=\"jiyegā maregā maregā\\nkaregā bharegā bharegā\\nare jiyegā maregā maregā karegā bharegā bharegā\\nsūlī chaṛhegā\\nlove ke lie sālā kuchh bhī karegā\\nare jiyegā maregā …\\njāgūṅ sote sote hṇs dūṅ rote rote\\ndil ko hārūṅ hārūṅ sīṭī mārūṅ mārūṅ\\nye nā jukām hai ye k‌yā nijām hai\\njinheṅ nā kām kuchh ye unkā kām hai\\nchalegā he he he he\\njo ho sahegā sahegā\\nāheṅ bharegā bharegā\\nuf nahīṅ karegā\\nlove ke lie …\\npalkeṅ jhapkūṅ jhapkūṅ\\nchhat se ṭapkūṅ ṭapkūṅ\\nāṇsū piyūṅ piyūṅ mar ke jiyūṅ jiyūṅ\\nhe palkeṅ jhap jhap jhapkūṅ\\nchhat se ṭap ṭap ṭapkūṅ\\nye nā mile kahīṅ ye nā khile kahīṅ\\nye aise chīr de jo nā sile kahīṅ\\nchalegā hā he he he he\\nbāp se laṛegā laṛegā\\nghar chhoṛ degā chhoṛegā\\ndil meṅ rahegā\\nlove ke lie …\\nare jiyegā maregā maregā karegā bharegā bharegā\\nsūlī chaṛhegā\\ndhan ke lie sālā kuchh bhī karegā\\ndhan dhan dhan dhanan dhan dhan dhan he dhan dhan dhan\\nchorī chorī chorī sīnā jorī jorī\\nḍālūṅ dānā dānā gāūṅ gānā gānā\\nkamīnā mān lo yā guṅḍā jān lo\\nharāmkhor hūṇ yā jo bhī nām lo\\nchalegā\\nchalega\\nbīvī se laṛegā laṛegā\\njel meṅ saṛegā saṛegā\\nare netā banegā\\ndhan ke lie sālā …\\nyārī yārī yārī mārā mārī mārī\\nronā dhonā dhonā jādū ṭonā ṭonā\\nhai ziṅdgī juā jo ho gayā huā\\nlagegī badduā chalegā\\nsālā kasāī maiṅ galī kā bhāī maiṅ\\n-+\\ngulām jorū kā nagar jamāī maiṅ banegā\\nāg pe chalegā chalegā\\npyār se ḍaregā ḍaregā\\ndil se bachegā\\ndhan ke lie sālā …\\nlove ke lie love ke lie\\nlove ke lie sālā kuchh bhī karegā\\nkuchh bhī karegā sālā kuchh bhī karegā\\nā\\nkuchh bhī karegā sālā\\nkuchh bhī karegā sālā kuchh bhī karegā\\nkaregā sālā bharegā\\nbharegā sālā maregā\\nkaregā bharegā bharegā maregā\\nlove ke lie re kuchh bhī karegā\\nā masjid ko jāte jāte maiṅ ruk gayā\\nkadmoṅ ko chūmne tere maiṅ jhuk gayā\\nkuchh bhī karegā sālā kuchh bhī karegā\\nna roje namāz na haz ko jāūṅ\\npāṇch vaqt us galī meṅ salām bhar āūṅ\\nai āheṅ bharegā sālā\\njo huā sahegā sālā\\nuf nahīṅ karegā sālā\\nkaregā bharegā sālā\\nbāp se laṛegā sālā\\nghar chhoṛ degā sālā\\ndil meṅ rahegā sālā\\nkuchh bhī karegā sālā kuchh bhī karegā\\nkisne pukārā kāfir ye kaisā sabab hai\\npūchh lo khudā se love bhī ik mazhab hai\\nlove ke lie love ke lie karegā kuchh bhī sālā\\nkaregā kuchh …\\nlove ke lie love ke lie love ke lie\\nkaregā kuchh bhī sālā love ke lie\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-qIW2VgvtdcL"
      },
      "outputs": [],
      "source": [
        "corpus=a.lower().split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGjSSbiNyKVY",
        "outputId": "b0c74fee-7052-4e2a-93a8-33d24b468aad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jiyegā maregā maregā',\n",
              " 'karegā bharegā bharegā',\n",
              " 'are jiyegā maregā maregā karegā bharegā bharegā',\n",
              " 'sūlī chaṛhegā',\n",
              " 'love ke lie sālā kuchh bhī karegā',\n",
              " 'are jiyegā maregā …',\n",
              " 'jāgūṅ sote sote hṇs dūṅ rote rote',\n",
              " 'dil ko hārūṅ hārūṅ sīṭī mārūṅ mārūṅ',\n",
              " 'ye nā jukām hai ye k\\u200cyā nijām hai',\n",
              " 'jinheṅ nā kām kuchh ye unkā kām hai',\n",
              " 'chalegā he he he he',\n",
              " 'jo ho sahegā sahegā',\n",
              " 'āheṅ bharegā bharegā',\n",
              " 'uf nahīṅ karegā',\n",
              " 'love ke lie …',\n",
              " 'palkeṅ jhapkūṅ jhapkūṅ',\n",
              " 'chhat se ṭapkūṅ ṭapkūṅ',\n",
              " 'āṇsū piyūṅ piyūṅ mar ke jiyūṅ jiyūṅ',\n",
              " 'he palkeṅ jhap jhap jhapkūṅ',\n",
              " 'chhat se ṭap ṭap ṭapkūṅ',\n",
              " 'ye nā mile kahīṅ ye nā khile kahīṅ',\n",
              " 'ye aise chīr de jo nā sile kahīṅ',\n",
              " 'chalegā hā he he he he',\n",
              " 'bāp se laṛegā laṛegā',\n",
              " 'ghar chhoṛ degā chhoṛegā',\n",
              " 'dil meṅ rahegā',\n",
              " 'love ke lie …',\n",
              " 'are jiyegā maregā maregā karegā bharegā bharegā',\n",
              " 'sūlī chaṛhegā',\n",
              " 'dhan ke lie sālā kuchh bhī karegā',\n",
              " 'dhan dhan dhan dhanan dhan dhan dhan he dhan dhan dhan',\n",
              " 'chorī chorī chorī sīnā jorī jorī',\n",
              " 'ḍālūṅ dānā dānā gāūṅ gānā gānā',\n",
              " 'kamīnā mān lo yā guṅḍā jān lo',\n",
              " 'harāmkhor hūṇ yā jo bhī nām lo',\n",
              " 'chalegā',\n",
              " 'chalega',\n",
              " 'bīvī se laṛegā laṛegā',\n",
              " 'jel meṅ saṛegā saṛegā',\n",
              " 'are netā banegā',\n",
              " 'dhan ke lie sālā …',\n",
              " 'yārī yārī yārī mārā mārī mārī',\n",
              " 'ronā dhonā dhonā jādū ṭonā ṭonā',\n",
              " 'hai ziṅdgī juā jo ho gayā huā',\n",
              " 'lagegī badduā chalegā',\n",
              " 'sālā kasāī maiṅ galī kā bhāī maiṅ',\n",
              " '-+',\n",
              " 'gulām jorū kā nagar jamāī maiṅ banegā',\n",
              " 'āg pe chalegā chalegā',\n",
              " 'pyār se ḍaregā ḍaregā',\n",
              " 'dil se bachegā',\n",
              " 'dhan ke lie sālā …',\n",
              " 'love ke lie love ke lie',\n",
              " 'love ke lie sālā kuchh bhī karegā',\n",
              " 'kuchh bhī karegā sālā kuchh bhī karegā',\n",
              " 'ā',\n",
              " 'kuchh bhī karegā sālā',\n",
              " 'kuchh bhī karegā sālā kuchh bhī karegā',\n",
              " 'karegā sālā bharegā',\n",
              " 'bharegā sālā maregā',\n",
              " 'karegā bharegā bharegā maregā',\n",
              " 'love ke lie re kuchh bhī karegā',\n",
              " 'ā masjid ko jāte jāte maiṅ ruk gayā',\n",
              " 'kadmoṅ ko chūmne tere maiṅ jhuk gayā',\n",
              " 'kuchh bhī karegā sālā kuchh bhī karegā',\n",
              " 'na roje namāz na haz ko jāūṅ',\n",
              " 'pāṇch vaqt us galī meṅ salām bhar āūṅ',\n",
              " 'ai āheṅ bharegā sālā',\n",
              " 'jo huā sahegā sālā',\n",
              " 'uf nahīṅ karegā sālā',\n",
              " 'karegā bharegā sālā',\n",
              " 'bāp se laṛegā sālā',\n",
              " 'ghar chhoṛ degā sālā',\n",
              " 'dil meṅ rahegā sālā',\n",
              " 'kuchh bhī karegā sālā kuchh bhī karegā',\n",
              " 'kisne pukārā kāfir ye kaisā sabab hai',\n",
              " 'pūchh lo khudā se love bhī ik mazhab hai',\n",
              " 'love ke lie love ke lie karegā kuchh bhī sālā',\n",
              " 'karegā kuchh …',\n",
              " 'love ke lie love ke lie love ke lie',\n",
              " 'karegā kuchh bhī sālā love ke lie']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MiMEaJMqtfQT"
      },
      "outputs": [],
      "source": [
        "b=Tokenizer(num_words=100)\n",
        "b.fit_on_texts(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O33vRyWI0JLX",
        "outputId": "a7633c98-2cea-4004-9b1e-0cdb9b36518c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.legacy.preprocessing.text.Tokenizer at 0x7a5fe020e8f0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQGqYrt-0Kbv",
        "outputId": "95e0ba91-baf8-4b47-d1da-e49dbad406f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('j', 34),\n",
              "             ('i', 52),\n",
              "             ('y', 25),\n",
              "             ('e', 186),\n",
              "             ('g', 98),\n",
              "             ('ā', 207),\n",
              "             ('m', 41),\n",
              "             ('a', 154),\n",
              "             ('r', 89),\n",
              "             ('k', 93),\n",
              "             ('b', 43),\n",
              "             ('h', 164),\n",
              "             ('s', 52),\n",
              "             ('ū', 28),\n",
              "             ('l', 86),\n",
              "             ('ī', 48),\n",
              "             ('c', 39),\n",
              "             ('ṛ', 12),\n",
              "             ('o', 50),\n",
              "             ('v', 16),\n",
              "             ('u', 32),\n",
              "             ('…', 6),\n",
              "             ('ṅ', 42),\n",
              "             ('t', 11),\n",
              "             ('ṇ', 4),\n",
              "             ('d', 32),\n",
              "             ('ṭ', 8),\n",
              "             ('n', 47),\n",
              "             ('\\u200c', 1),\n",
              "             ('f', 3),\n",
              "             ('p', 21),\n",
              "             ('ḍ', 4),\n",
              "             ('z', 4),\n",
              "             ('q', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "b.word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9qBF9UYr0R4v"
      },
      "outputs": [],
      "source": [
        "tn=Tokenizer()\n",
        "tn.fit_on_texts(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gV_w2QH-JII4"
      },
      "outputs": [],
      "source": [
        "totalwords=len(tn.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynkHDmMQJUE_",
        "outputId": "59962b75-252c-441a-84a1-90b7a61ec2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n",
            "{'karegā': 1, 'sālā': 2, 'ke': 3, 'kuchh': 4, 'bhī': 5, 'lie': 6, 'bharegā': 7, 'love': 8, 'dhan': 9, 'he': 10, 'maregā': 11, 'se': 12, 'ye': 13, '…': 14, 'hai': 15, 'chalegā': 16, 'nā': 17, 'jo': 18, 'laṛegā': 19, 'maiṅ': 20, 'jiyegā': 21, 'are': 22, 'dil': 23, 'ko': 24, 'meṅ': 25, 'lo': 26, 'sahegā': 27, 'jhapkūṅ': 28, 'ṭapkūṅ': 29, 'kahīṅ': 30, 'chorī': 31, 'yārī': 32, 'gayā': 33, 'sūlī': 34, 'chaṛhegā': 35, 'sote': 36, 'rote': 37, 'hārūṅ': 38, 'mārūṅ': 39, 'kām': 40, 'ho': 41, 'āheṅ': 42, 'uf': 43, 'nahīṅ': 44, 'palkeṅ': 45, 'chhat': 46, 'piyūṅ': 47, 'jiyūṅ': 48, 'jhap': 49, 'ṭap': 50, 'bāp': 51, 'ghar': 52, 'chhoṛ': 53, 'degā': 54, 'rahegā': 55, 'jorī': 56, 'dānā': 57, 'gānā': 58, 'yā': 59, 'saṛegā': 60, 'banegā': 61, 'mārī': 62, 'dhonā': 63, 'ṭonā': 64, 'huā': 65, 'galī': 66, 'kā': 67, 'ḍaregā': 68, 'ā': 69, 'jāte': 70, 'na': 71, 'jāgūṅ': 72, 'hṇs': 73, 'dūṅ': 74, 'sīṭī': 75, 'jukām': 76, 'k\\u200cyā': 77, 'nijām': 78, 'jinheṅ': 79, 'unkā': 80, 'āṇsū': 81, 'mar': 82, 'mile': 83, 'khile': 84, 'aise': 85, 'chīr': 86, 'de': 87, 'sile': 88, 'hā': 89, 'chhoṛegā': 90, 'dhanan': 91, 'sīnā': 92, 'ḍālūṅ': 93, 'gāūṅ': 94, 'kamīnā': 95, 'mān': 96, 'guṅḍā': 97, 'jān': 98, 'harāmkhor': 99, 'hūṇ': 100, 'nām': 101, 'chalega': 102, 'bīvī': 103, 'jel': 104, 'netā': 105, 'mārā': 106, 'ronā': 107, 'jādū': 108, 'ziṅdgī': 109, 'juā': 110, 'lagegī': 111, 'badduā': 112, 'kasāī': 113, 'bhāī': 114, 'gulām': 115, 'jorū': 116, 'nagar': 117, 'jamāī': 118, 'āg': 119, 'pe': 120, 'pyār': 121, 'bachegā': 122, 're': 123, 'masjid': 124, 'ruk': 125, 'kadmoṅ': 126, 'chūmne': 127, 'tere': 128, 'jhuk': 129, 'roje': 130, 'namāz': 131, 'haz': 132, 'jāūṅ': 133, 'pāṇch': 134, 'vaqt': 135, 'us': 136, 'salām': 137, 'bhar': 138, 'āūṅ': 139, 'ai': 140, 'kisne': 141, 'pukārā': 142, 'kāfir': 143, 'kaisā': 144, 'sabab': 145, 'pūchh': 146, 'khudā': 147, 'ik': 148, 'mazhab': 149}\n"
          ]
        }
      ],
      "source": [
        "print(totalwords)\n",
        "print(tn.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fXkd-Z6OJZCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0582043b-3e1d-4def-82b3-4aa99082436a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21, 11]\n",
            "[21, 11, 11]\n",
            "[1, 7]\n",
            "[1, 7, 7]\n",
            "[22, 21]\n",
            "[22, 21, 11]\n",
            "[22, 21, 11, 11]\n",
            "[22, 21, 11, 11, 1]\n",
            "[22, 21, 11, 11, 1, 7]\n",
            "[22, 21, 11, 11, 1, 7, 7]\n",
            "[34, 35]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 2]\n",
            "[8, 3, 6, 2, 4]\n",
            "[8, 3, 6, 2, 4, 5]\n",
            "[8, 3, 6, 2, 4, 5, 1]\n",
            "[22, 21]\n",
            "[22, 21, 11]\n",
            "[22, 21, 11, 14]\n",
            "[72, 36]\n",
            "[72, 36, 36]\n",
            "[72, 36, 36, 73]\n",
            "[72, 36, 36, 73, 74]\n",
            "[72, 36, 36, 73, 74, 37]\n",
            "[72, 36, 36, 73, 74, 37, 37]\n",
            "[23, 24]\n",
            "[23, 24, 38]\n",
            "[23, 24, 38, 38]\n",
            "[23, 24, 38, 38, 75]\n",
            "[23, 24, 38, 38, 75, 39]\n",
            "[23, 24, 38, 38, 75, 39, 39]\n",
            "[13, 17]\n",
            "[13, 17, 76]\n",
            "[13, 17, 76, 15]\n",
            "[13, 17, 76, 15, 13]\n",
            "[13, 17, 76, 15, 13, 77]\n",
            "[13, 17, 76, 15, 13, 77, 78]\n",
            "[13, 17, 76, 15, 13, 77, 78, 15]\n",
            "[79, 17]\n",
            "[79, 17, 40]\n",
            "[79, 17, 40, 4]\n",
            "[79, 17, 40, 4, 13]\n",
            "[79, 17, 40, 4, 13, 80]\n",
            "[79, 17, 40, 4, 13, 80, 40]\n",
            "[79, 17, 40, 4, 13, 80, 40, 15]\n",
            "[16, 10]\n",
            "[16, 10, 10]\n",
            "[16, 10, 10, 10]\n",
            "[16, 10, 10, 10, 10]\n",
            "[18, 41]\n",
            "[18, 41, 27]\n",
            "[18, 41, 27, 27]\n",
            "[42, 7]\n",
            "[42, 7, 7]\n",
            "[43, 44]\n",
            "[43, 44, 1]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 14]\n",
            "[45, 28]\n",
            "[45, 28, 28]\n",
            "[46, 12]\n",
            "[46, 12, 29]\n",
            "[46, 12, 29, 29]\n",
            "[81, 47]\n",
            "[81, 47, 47]\n",
            "[81, 47, 47, 82]\n",
            "[81, 47, 47, 82, 3]\n",
            "[81, 47, 47, 82, 3, 48]\n",
            "[81, 47, 47, 82, 3, 48, 48]\n",
            "[10, 45]\n",
            "[10, 45, 49]\n",
            "[10, 45, 49, 49]\n",
            "[10, 45, 49, 49, 28]\n",
            "[46, 12]\n",
            "[46, 12, 50]\n",
            "[46, 12, 50, 50]\n",
            "[46, 12, 50, 50, 29]\n",
            "[13, 17]\n",
            "[13, 17, 83]\n",
            "[13, 17, 83, 30]\n",
            "[13, 17, 83, 30, 13]\n",
            "[13, 17, 83, 30, 13, 17]\n",
            "[13, 17, 83, 30, 13, 17, 84]\n",
            "[13, 17, 83, 30, 13, 17, 84, 30]\n",
            "[13, 85]\n",
            "[13, 85, 86]\n",
            "[13, 85, 86, 87]\n",
            "[13, 85, 86, 87, 18]\n",
            "[13, 85, 86, 87, 18, 17]\n",
            "[13, 85, 86, 87, 18, 17, 88]\n",
            "[13, 85, 86, 87, 18, 17, 88, 30]\n",
            "[16, 89]\n",
            "[16, 89, 10]\n",
            "[16, 89, 10, 10]\n",
            "[16, 89, 10, 10, 10]\n",
            "[16, 89, 10, 10, 10, 10]\n",
            "[51, 12]\n",
            "[51, 12, 19]\n",
            "[51, 12, 19, 19]\n",
            "[52, 53]\n",
            "[52, 53, 54]\n",
            "[52, 53, 54, 90]\n",
            "[23, 25]\n",
            "[23, 25, 55]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 14]\n",
            "[22, 21]\n",
            "[22, 21, 11]\n",
            "[22, 21, 11, 11]\n",
            "[22, 21, 11, 11, 1]\n",
            "[22, 21, 11, 11, 1, 7]\n",
            "[22, 21, 11, 11, 1, 7, 7]\n",
            "[34, 35]\n",
            "[9, 3]\n",
            "[9, 3, 6]\n",
            "[9, 3, 6, 2]\n",
            "[9, 3, 6, 2, 4]\n",
            "[9, 3, 6, 2, 4, 5]\n",
            "[9, 3, 6, 2, 4, 5, 1]\n",
            "[9, 9]\n",
            "[9, 9, 9]\n",
            "[9, 9, 9, 91]\n",
            "[9, 9, 9, 91, 9]\n",
            "[9, 9, 9, 91, 9, 9]\n",
            "[9, 9, 9, 91, 9, 9, 9]\n",
            "[9, 9, 9, 91, 9, 9, 9, 10]\n",
            "[9, 9, 9, 91, 9, 9, 9, 10, 9]\n",
            "[9, 9, 9, 91, 9, 9, 9, 10, 9, 9]\n",
            "[9, 9, 9, 91, 9, 9, 9, 10, 9, 9, 9]\n",
            "[31, 31]\n",
            "[31, 31, 31]\n",
            "[31, 31, 31, 92]\n",
            "[31, 31, 31, 92, 56]\n",
            "[31, 31, 31, 92, 56, 56]\n",
            "[93, 57]\n",
            "[93, 57, 57]\n",
            "[93, 57, 57, 94]\n",
            "[93, 57, 57, 94, 58]\n",
            "[93, 57, 57, 94, 58, 58]\n",
            "[95, 96]\n",
            "[95, 96, 26]\n",
            "[95, 96, 26, 59]\n",
            "[95, 96, 26, 59, 97]\n",
            "[95, 96, 26, 59, 97, 98]\n",
            "[95, 96, 26, 59, 97, 98, 26]\n",
            "[99, 100]\n",
            "[99, 100, 59]\n",
            "[99, 100, 59, 18]\n",
            "[99, 100, 59, 18, 5]\n",
            "[99, 100, 59, 18, 5, 101]\n",
            "[99, 100, 59, 18, 5, 101, 26]\n",
            "[103, 12]\n",
            "[103, 12, 19]\n",
            "[103, 12, 19, 19]\n",
            "[104, 25]\n",
            "[104, 25, 60]\n",
            "[104, 25, 60, 60]\n",
            "[22, 105]\n",
            "[22, 105, 61]\n",
            "[9, 3]\n",
            "[9, 3, 6]\n",
            "[9, 3, 6, 2]\n",
            "[9, 3, 6, 2, 14]\n",
            "[32, 32]\n",
            "[32, 32, 32]\n",
            "[32, 32, 32, 106]\n",
            "[32, 32, 32, 106, 62]\n",
            "[32, 32, 32, 106, 62, 62]\n",
            "[107, 63]\n",
            "[107, 63, 63]\n",
            "[107, 63, 63, 108]\n",
            "[107, 63, 63, 108, 64]\n",
            "[107, 63, 63, 108, 64, 64]\n",
            "[15, 109]\n",
            "[15, 109, 110]\n",
            "[15, 109, 110, 18]\n",
            "[15, 109, 110, 18, 41]\n",
            "[15, 109, 110, 18, 41, 33]\n",
            "[15, 109, 110, 18, 41, 33, 65]\n",
            "[111, 112]\n",
            "[111, 112, 16]\n",
            "[2, 113]\n",
            "[2, 113, 20]\n",
            "[2, 113, 20, 66]\n",
            "[2, 113, 20, 66, 67]\n",
            "[2, 113, 20, 66, 67, 114]\n",
            "[2, 113, 20, 66, 67, 114, 20]\n",
            "[115, 116]\n",
            "[115, 116, 67]\n",
            "[115, 116, 67, 117]\n",
            "[115, 116, 67, 117, 118]\n",
            "[115, 116, 67, 117, 118, 20]\n",
            "[115, 116, 67, 117, 118, 20, 61]\n",
            "[119, 120]\n",
            "[119, 120, 16]\n",
            "[119, 120, 16, 16]\n",
            "[121, 12]\n",
            "[121, 12, 68]\n",
            "[121, 12, 68, 68]\n",
            "[23, 12]\n",
            "[23, 12, 122]\n",
            "[9, 3]\n",
            "[9, 3, 6]\n",
            "[9, 3, 6, 2]\n",
            "[9, 3, 6, 2, 14]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 8]\n",
            "[8, 3, 6, 8, 3]\n",
            "[8, 3, 6, 8, 3, 6]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 2]\n",
            "[8, 3, 6, 2, 4]\n",
            "[8, 3, 6, 2, 4, 5]\n",
            "[8, 3, 6, 2, 4, 5, 1]\n",
            "[4, 5]\n",
            "[4, 5, 1]\n",
            "[4, 5, 1, 2]\n",
            "[4, 5, 1, 2, 4]\n",
            "[4, 5, 1, 2, 4, 5]\n",
            "[4, 5, 1, 2, 4, 5, 1]\n",
            "[4, 5]\n",
            "[4, 5, 1]\n",
            "[4, 5, 1, 2]\n",
            "[4, 5]\n",
            "[4, 5, 1]\n",
            "[4, 5, 1, 2]\n",
            "[4, 5, 1, 2, 4]\n",
            "[4, 5, 1, 2, 4, 5]\n",
            "[4, 5, 1, 2, 4, 5, 1]\n",
            "[1, 2]\n",
            "[1, 2, 7]\n",
            "[7, 2]\n",
            "[7, 2, 11]\n",
            "[1, 7]\n",
            "[1, 7, 7]\n",
            "[1, 7, 7, 11]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 123]\n",
            "[8, 3, 6, 123, 4]\n",
            "[8, 3, 6, 123, 4, 5]\n",
            "[8, 3, 6, 123, 4, 5, 1]\n",
            "[69, 124]\n",
            "[69, 124, 24]\n",
            "[69, 124, 24, 70]\n",
            "[69, 124, 24, 70, 70]\n",
            "[69, 124, 24, 70, 70, 20]\n",
            "[69, 124, 24, 70, 70, 20, 125]\n",
            "[69, 124, 24, 70, 70, 20, 125, 33]\n",
            "[126, 24]\n",
            "[126, 24, 127]\n",
            "[126, 24, 127, 128]\n",
            "[126, 24, 127, 128, 20]\n",
            "[126, 24, 127, 128, 20, 129]\n",
            "[126, 24, 127, 128, 20, 129, 33]\n",
            "[4, 5]\n",
            "[4, 5, 1]\n",
            "[4, 5, 1, 2]\n",
            "[4, 5, 1, 2, 4]\n",
            "[4, 5, 1, 2, 4, 5]\n",
            "[4, 5, 1, 2, 4, 5, 1]\n",
            "[71, 130]\n",
            "[71, 130, 131]\n",
            "[71, 130, 131, 71]\n",
            "[71, 130, 131, 71, 132]\n",
            "[71, 130, 131, 71, 132, 24]\n",
            "[71, 130, 131, 71, 132, 24, 133]\n",
            "[134, 135]\n",
            "[134, 135, 136]\n",
            "[134, 135, 136, 66]\n",
            "[134, 135, 136, 66, 25]\n",
            "[134, 135, 136, 66, 25, 137]\n",
            "[134, 135, 136, 66, 25, 137, 138]\n",
            "[134, 135, 136, 66, 25, 137, 138, 139]\n",
            "[140, 42]\n",
            "[140, 42, 7]\n",
            "[140, 42, 7, 2]\n",
            "[18, 65]\n",
            "[18, 65, 27]\n",
            "[18, 65, 27, 2]\n",
            "[43, 44]\n",
            "[43, 44, 1]\n",
            "[43, 44, 1, 2]\n",
            "[1, 7]\n",
            "[1, 7, 2]\n",
            "[51, 12]\n",
            "[51, 12, 19]\n",
            "[51, 12, 19, 2]\n",
            "[52, 53]\n",
            "[52, 53, 54]\n",
            "[52, 53, 54, 2]\n",
            "[23, 25]\n",
            "[23, 25, 55]\n",
            "[23, 25, 55, 2]\n",
            "[4, 5]\n",
            "[4, 5, 1]\n",
            "[4, 5, 1, 2]\n",
            "[4, 5, 1, 2, 4]\n",
            "[4, 5, 1, 2, 4, 5]\n",
            "[4, 5, 1, 2, 4, 5, 1]\n",
            "[141, 142]\n",
            "[141, 142, 143]\n",
            "[141, 142, 143, 13]\n",
            "[141, 142, 143, 13, 144]\n",
            "[141, 142, 143, 13, 144, 145]\n",
            "[141, 142, 143, 13, 144, 145, 15]\n",
            "[146, 26]\n",
            "[146, 26, 147]\n",
            "[146, 26, 147, 12]\n",
            "[146, 26, 147, 12, 8]\n",
            "[146, 26, 147, 12, 8, 5]\n",
            "[146, 26, 147, 12, 8, 5, 148]\n",
            "[146, 26, 147, 12, 8, 5, 148, 149]\n",
            "[146, 26, 147, 12, 8, 5, 148, 149, 15]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 8]\n",
            "[8, 3, 6, 8, 3]\n",
            "[8, 3, 6, 8, 3, 6]\n",
            "[8, 3, 6, 8, 3, 6, 1]\n",
            "[8, 3, 6, 8, 3, 6, 1, 4]\n",
            "[8, 3, 6, 8, 3, 6, 1, 4, 5]\n",
            "[8, 3, 6, 8, 3, 6, 1, 4, 5, 2]\n",
            "[1, 4]\n",
            "[1, 4, 14]\n",
            "[8, 3]\n",
            "[8, 3, 6]\n",
            "[8, 3, 6, 8]\n",
            "[8, 3, 6, 8, 3]\n",
            "[8, 3, 6, 8, 3, 6]\n",
            "[8, 3, 6, 8, 3, 6, 8]\n",
            "[8, 3, 6, 8, 3, 6, 8, 3]\n",
            "[8, 3, 6, 8, 3, 6, 8, 3, 6]\n",
            "[1, 4]\n",
            "[1, 4, 5]\n",
            "[1, 4, 5, 2]\n",
            "[1, 4, 5, 2, 8]\n",
            "[1, 4, 5, 2, 8, 3]\n",
            "[1, 4, 5, 2, 8, 3, 6]\n"
          ]
        }
      ],
      "source": [
        "input_sequences=[]\n",
        "for line in corpus:\n",
        "    token_list=tn.texts_to_sequences([line])[0]\n",
        "    for i in range(1,len(token_list)):\n",
        "        n_gram_sequence=token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "        print(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUd5u_ZyLQiY",
        "outputId": "a30ba3b0-eb6d-4f0f-c288-fb7da591b752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21, 11], [21, 11, 11], [1, 7], [1, 7, 7], [22, 21], [22, 21, 11], [22, 21, 11, 11], [22, 21, 11, 11, 1], [22, 21, 11, 11, 1, 7], [22, 21, 11, 11, 1, 7, 7], [34, 35], [8, 3], [8, 3, 6], [8, 3, 6, 2], [8, 3, 6, 2, 4], [8, 3, 6, 2, 4, 5], [8, 3, 6, 2, 4, 5, 1], [22, 21], [22, 21, 11], [22, 21, 11, 14], [72, 36], [72, 36, 36], [72, 36, 36, 73], [72, 36, 36, 73, 74], [72, 36, 36, 73, 74, 37], [72, 36, 36, 73, 74, 37, 37], [23, 24], [23, 24, 38], [23, 24, 38, 38], [23, 24, 38, 38, 75], [23, 24, 38, 38, 75, 39], [23, 24, 38, 38, 75, 39, 39], [13, 17], [13, 17, 76], [13, 17, 76, 15], [13, 17, 76, 15, 13], [13, 17, 76, 15, 13, 77], [13, 17, 76, 15, 13, 77, 78], [13, 17, 76, 15, 13, 77, 78, 15], [79, 17], [79, 17, 40], [79, 17, 40, 4], [79, 17, 40, 4, 13], [79, 17, 40, 4, 13, 80], [79, 17, 40, 4, 13, 80, 40], [79, 17, 40, 4, 13, 80, 40, 15], [16, 10], [16, 10, 10], [16, 10, 10, 10], [16, 10, 10, 10, 10], [18, 41], [18, 41, 27], [18, 41, 27, 27], [42, 7], [42, 7, 7], [43, 44], [43, 44, 1], [8, 3], [8, 3, 6], [8, 3, 6, 14], [45, 28], [45, 28, 28], [46, 12], [46, 12, 29], [46, 12, 29, 29], [81, 47], [81, 47, 47], [81, 47, 47, 82], [81, 47, 47, 82, 3], [81, 47, 47, 82, 3, 48], [81, 47, 47, 82, 3, 48, 48], [10, 45], [10, 45, 49], [10, 45, 49, 49], [10, 45, 49, 49, 28], [46, 12], [46, 12, 50], [46, 12, 50, 50], [46, 12, 50, 50, 29], [13, 17], [13, 17, 83], [13, 17, 83, 30], [13, 17, 83, 30, 13], [13, 17, 83, 30, 13, 17], [13, 17, 83, 30, 13, 17, 84], [13, 17, 83, 30, 13, 17, 84, 30], [13, 85], [13, 85, 86], [13, 85, 86, 87], [13, 85, 86, 87, 18], [13, 85, 86, 87, 18, 17], [13, 85, 86, 87, 18, 17, 88], [13, 85, 86, 87, 18, 17, 88, 30], [16, 89], [16, 89, 10], [16, 89, 10, 10], [16, 89, 10, 10, 10], [16, 89, 10, 10, 10, 10], [51, 12], [51, 12, 19], [51, 12, 19, 19], [52, 53], [52, 53, 54], [52, 53, 54, 90], [23, 25], [23, 25, 55], [8, 3], [8, 3, 6], [8, 3, 6, 14], [22, 21], [22, 21, 11], [22, 21, 11, 11], [22, 21, 11, 11, 1], [22, 21, 11, 11, 1, 7], [22, 21, 11, 11, 1, 7, 7], [34, 35], [9, 3], [9, 3, 6], [9, 3, 6, 2], [9, 3, 6, 2, 4], [9, 3, 6, 2, 4, 5], [9, 3, 6, 2, 4, 5, 1], [9, 9], [9, 9, 9], [9, 9, 9, 91], [9, 9, 9, 91, 9], [9, 9, 9, 91, 9, 9], [9, 9, 9, 91, 9, 9, 9], [9, 9, 9, 91, 9, 9, 9, 10], [9, 9, 9, 91, 9, 9, 9, 10, 9], [9, 9, 9, 91, 9, 9, 9, 10, 9, 9], [9, 9, 9, 91, 9, 9, 9, 10, 9, 9, 9], [31, 31], [31, 31, 31], [31, 31, 31, 92], [31, 31, 31, 92, 56], [31, 31, 31, 92, 56, 56], [93, 57], [93, 57, 57], [93, 57, 57, 94], [93, 57, 57, 94, 58], [93, 57, 57, 94, 58, 58], [95, 96], [95, 96, 26], [95, 96, 26, 59], [95, 96, 26, 59, 97], [95, 96, 26, 59, 97, 98], [95, 96, 26, 59, 97, 98, 26], [99, 100], [99, 100, 59], [99, 100, 59, 18], [99, 100, 59, 18, 5], [99, 100, 59, 18, 5, 101], [99, 100, 59, 18, 5, 101, 26], [103, 12], [103, 12, 19], [103, 12, 19, 19], [104, 25], [104, 25, 60], [104, 25, 60, 60], [22, 105], [22, 105, 61], [9, 3], [9, 3, 6], [9, 3, 6, 2], [9, 3, 6, 2, 14], [32, 32], [32, 32, 32], [32, 32, 32, 106], [32, 32, 32, 106, 62], [32, 32, 32, 106, 62, 62], [107, 63], [107, 63, 63], [107, 63, 63, 108], [107, 63, 63, 108, 64], [107, 63, 63, 108, 64, 64], [15, 109], [15, 109, 110], [15, 109, 110, 18], [15, 109, 110, 18, 41], [15, 109, 110, 18, 41, 33], [15, 109, 110, 18, 41, 33, 65], [111, 112], [111, 112, 16], [2, 113], [2, 113, 20], [2, 113, 20, 66], [2, 113, 20, 66, 67], [2, 113, 20, 66, 67, 114], [2, 113, 20, 66, 67, 114, 20], [115, 116], [115, 116, 67], [115, 116, 67, 117], [115, 116, 67, 117, 118], [115, 116, 67, 117, 118, 20], [115, 116, 67, 117, 118, 20, 61], [119, 120], [119, 120, 16], [119, 120, 16, 16], [121, 12], [121, 12, 68], [121, 12, 68, 68], [23, 12], [23, 12, 122], [9, 3], [9, 3, 6], [9, 3, 6, 2], [9, 3, 6, 2, 14], [8, 3], [8, 3, 6], [8, 3, 6, 8], [8, 3, 6, 8, 3], [8, 3, 6, 8, 3, 6], [8, 3], [8, 3, 6], [8, 3, 6, 2], [8, 3, 6, 2, 4], [8, 3, 6, 2, 4, 5], [8, 3, 6, 2, 4, 5, 1], [4, 5], [4, 5, 1], [4, 5, 1, 2], [4, 5, 1, 2, 4], [4, 5, 1, 2, 4, 5], [4, 5, 1, 2, 4, 5, 1], [4, 5], [4, 5, 1], [4, 5, 1, 2], [4, 5], [4, 5, 1], [4, 5, 1, 2], [4, 5, 1, 2, 4], [4, 5, 1, 2, 4, 5], [4, 5, 1, 2, 4, 5, 1], [1, 2], [1, 2, 7], [7, 2], [7, 2, 11], [1, 7], [1, 7, 7], [1, 7, 7, 11], [8, 3], [8, 3, 6], [8, 3, 6, 123], [8, 3, 6, 123, 4], [8, 3, 6, 123, 4, 5], [8, 3, 6, 123, 4, 5, 1], [69, 124], [69, 124, 24], [69, 124, 24, 70], [69, 124, 24, 70, 70], [69, 124, 24, 70, 70, 20], [69, 124, 24, 70, 70, 20, 125], [69, 124, 24, 70, 70, 20, 125, 33], [126, 24], [126, 24, 127], [126, 24, 127, 128], [126, 24, 127, 128, 20], [126, 24, 127, 128, 20, 129], [126, 24, 127, 128, 20, 129, 33], [4, 5], [4, 5, 1], [4, 5, 1, 2], [4, 5, 1, 2, 4], [4, 5, 1, 2, 4, 5], [4, 5, 1, 2, 4, 5, 1], [71, 130], [71, 130, 131], [71, 130, 131, 71], [71, 130, 131, 71, 132], [71, 130, 131, 71, 132, 24], [71, 130, 131, 71, 132, 24, 133], [134, 135], [134, 135, 136], [134, 135, 136, 66], [134, 135, 136, 66, 25], [134, 135, 136, 66, 25, 137], [134, 135, 136, 66, 25, 137, 138], [134, 135, 136, 66, 25, 137, 138, 139], [140, 42], [140, 42, 7], [140, 42, 7, 2], [18, 65], [18, 65, 27], [18, 65, 27, 2], [43, 44], [43, 44, 1], [43, 44, 1, 2], [1, 7], [1, 7, 2], [51, 12], [51, 12, 19], [51, 12, 19, 2], [52, 53], [52, 53, 54], [52, 53, 54, 2], [23, 25], [23, 25, 55], [23, 25, 55, 2], [4, 5], [4, 5, 1], [4, 5, 1, 2], [4, 5, 1, 2, 4], [4, 5, 1, 2, 4, 5], [4, 5, 1, 2, 4, 5, 1], [141, 142], [141, 142, 143], [141, 142, 143, 13], [141, 142, 143, 13, 144], [141, 142, 143, 13, 144, 145], [141, 142, 143, 13, 144, 145, 15], [146, 26], [146, 26, 147], [146, 26, 147, 12], [146, 26, 147, 12, 8], [146, 26, 147, 12, 8, 5], [146, 26, 147, 12, 8, 5, 148], [146, 26, 147, 12, 8, 5, 148, 149], [146, 26, 147, 12, 8, 5, 148, 149, 15], [8, 3], [8, 3, 6], [8, 3, 6, 8], [8, 3, 6, 8, 3], [8, 3, 6, 8, 3, 6], [8, 3, 6, 8, 3, 6, 1], [8, 3, 6, 8, 3, 6, 1, 4], [8, 3, 6, 8, 3, 6, 1, 4, 5], [8, 3, 6, 8, 3, 6, 1, 4, 5, 2], [1, 4], [1, 4, 14], [8, 3], [8, 3, 6], [8, 3, 6, 8], [8, 3, 6, 8, 3], [8, 3, 6, 8, 3, 6], [8, 3, 6, 8, 3, 6, 8], [8, 3, 6, 8, 3, 6, 8, 3], [8, 3, 6, 8, 3, 6, 8, 3, 6], [1, 4], [1, 4, 5], [1, 4, 5, 2], [1, 4, 5, 2, 8], [1, 4, 5, 2, 8, 3], [1, 4, 5, 2, 8, 3, 6]]\n"
          ]
        }
      ],
      "source": [
        "print(input_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "d708vscUKh6X"
      },
      "outputs": [],
      "source": [
        "max_sequence_len=max([len(x) for x in input_sequences])\n",
        "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt2ZDsBsLuiF",
        "outputId": "afbc3a11-5816-42cb-b3a2-f01716ca571a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "344\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "print(len(input_sequences))\n",
        "print(max_sequence_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lOdi2JSvLxVc"
      },
      "outputs": [],
      "source": [
        "xs,labels=input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys=tf.keras.utils.to_categorical(labels,num_classes=totalwords)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(totalwords,100,input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "\n",
        "model.add(Dense(totalwords,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "2YJcy0bIRgG-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ak=model.fit(xs,ys,epochs=100,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8dCn3aXaDgO",
        "outputId": "0df9c208-da00-4d3e-bd6b-4f409d27d890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.0372 - loss: 5.0041\n",
            "Epoch 2/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0447 - loss: 4.8925\n",
            "Epoch 3/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0543 - loss: 4.5034\n",
            "Epoch 4/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0541 - loss: 4.3487\n",
            "Epoch 5/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1039 - loss: 4.1913\n",
            "Epoch 6/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0870 - loss: 4.2923\n",
            "Epoch 7/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0543 - loss: 4.1961\n",
            "Epoch 8/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0885 - loss: 4.1096\n",
            "Epoch 9/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1097 - loss: 3.9443\n",
            "Epoch 10/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1178 - loss: 3.8159\n",
            "Epoch 11/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.1446 - loss: 3.6775\n",
            "Epoch 12/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1497 - loss: 3.5362\n",
            "Epoch 13/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1682 - loss: 3.4857\n",
            "Epoch 14/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2162 - loss: 3.4128\n",
            "Epoch 15/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1897 - loss: 3.3108\n",
            "Epoch 16/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2179 - loss: 3.2475\n",
            "Epoch 17/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2180 - loss: 3.0490\n",
            "Epoch 18/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.2307 - loss: 2.9716\n",
            "Epoch 19/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2465 - loss: 2.9514\n",
            "Epoch 20/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2478 - loss: 2.9197\n",
            "Epoch 21/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2807 - loss: 2.7985\n",
            "Epoch 22/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2999 - loss: 2.7794\n",
            "Epoch 23/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3527 - loss: 2.5855\n",
            "Epoch 24/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3760 - loss: 2.5567\n",
            "Epoch 25/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3660 - loss: 2.4524\n",
            "Epoch 26/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3446 - loss: 2.5931\n",
            "Epoch 27/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3416 - loss: 2.5005\n",
            "Epoch 28/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3953 - loss: 2.4270\n",
            "Epoch 29/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3879 - loss: 2.3618\n",
            "Epoch 30/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4233 - loss: 2.2251\n",
            "Epoch 31/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4332 - loss: 2.2135\n",
            "Epoch 32/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4634 - loss: 2.1319\n",
            "Epoch 33/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4825 - loss: 2.0966\n",
            "Epoch 34/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4650 - loss: 2.0621\n",
            "Epoch 35/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4470 - loss: 2.0710\n",
            "Epoch 36/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4697 - loss: 2.0248\n",
            "Epoch 37/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5187 - loss: 1.9326\n",
            "Epoch 38/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4751 - loss: 1.9677\n",
            "Epoch 39/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5062 - loss: 1.9401\n",
            "Epoch 40/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5269 - loss: 1.7969\n",
            "Epoch 41/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5263 - loss: 1.8453\n",
            "Epoch 42/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5075 - loss: 1.7959\n",
            "Epoch 43/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5183 - loss: 1.8014\n",
            "Epoch 44/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5712 - loss: 1.6667\n",
            "Epoch 45/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5885 - loss: 1.6085\n",
            "Epoch 46/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5855 - loss: 1.6788\n",
            "Epoch 47/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5540 - loss: 1.6088\n",
            "Epoch 48/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5989 - loss: 1.6076\n",
            "Epoch 49/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6019 - loss: 1.4928\n",
            "Epoch 50/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6151 - loss: 1.5193\n",
            "Epoch 51/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5760 - loss: 1.5346\n",
            "Epoch 52/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6175 - loss: 1.5329\n",
            "Epoch 53/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.6249 - loss: 1.4325\n",
            "Epoch 54/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.5956 - loss: 1.4648\n",
            "Epoch 55/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6185 - loss: 1.4198\n",
            "Epoch 56/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.5951 - loss: 1.4160\n",
            "Epoch 57/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6194 - loss: 1.4411\n",
            "Epoch 58/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5832 - loss: 1.4600\n",
            "Epoch 59/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6492 - loss: 1.3284\n",
            "Epoch 60/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6138 - loss: 1.3833\n",
            "Epoch 61/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6357 - loss: 1.2704\n",
            "Epoch 62/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6745 - loss: 1.2954\n",
            "Epoch 63/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6993 - loss: 1.2901\n",
            "Epoch 64/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6692 - loss: 1.2060\n",
            "Epoch 65/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6847 - loss: 1.2529\n",
            "Epoch 66/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6789 - loss: 1.2152\n",
            "Epoch 67/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7032 - loss: 1.1977\n",
            "Epoch 68/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6943 - loss: 1.1681\n",
            "Epoch 69/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7035 - loss: 1.1575\n",
            "Epoch 70/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6828 - loss: 1.1754\n",
            "Epoch 71/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6748 - loss: 1.1305\n",
            "Epoch 72/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6865 - loss: 1.1048\n",
            "Epoch 73/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6673 - loss: 1.1331\n",
            "Epoch 74/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7585 - loss: 1.0138\n",
            "Epoch 75/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7347 - loss: 1.0602\n",
            "Epoch 76/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7102 - loss: 1.0414\n",
            "Epoch 77/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6988 - loss: 1.0350\n",
            "Epoch 78/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7056 - loss: 1.0127\n",
            "Epoch 79/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7405 - loss: 0.9744\n",
            "Epoch 80/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7231 - loss: 0.9898\n",
            "Epoch 81/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7315 - loss: 1.0151\n",
            "Epoch 82/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7474 - loss: 0.9303\n",
            "Epoch 83/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7336 - loss: 0.9118\n",
            "Epoch 84/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7454 - loss: 0.9127\n",
            "Epoch 85/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7422 - loss: 0.9214\n",
            "Epoch 86/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7592 - loss: 0.9468\n",
            "Epoch 87/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7426 - loss: 0.9056\n",
            "Epoch 88/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7259 - loss: 0.9079\n",
            "Epoch 89/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7507 - loss: 0.8728\n",
            "Epoch 90/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7674 - loss: 0.8618\n",
            "Epoch 91/100\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7830 - loss: 0.8119\n",
            "Epoch 92/100\n",
            "\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6875 - loss: 1.0733"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluated=model.evaluate(xs,ys,verbose=0)"
      ],
      "metadata": {
        "id": "RNl5kv9SbScc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluated)"
      ],
      "metadata": {
        "id": "s9H4WjnVQx2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import pread\n",
        "seed_text=\"love ke lie\"\n",
        "next_words=10\n",
        "for _ in range(next_words):\n",
        "    token_list=tn.texts_to_sequences([seed_text])[0]\n",
        "    token_list=pad_sequences([token_list],maxlen=max_sequence_len-1,padding='post')\n",
        "    predicted_probs=model.predict(token_list,verbose=0)\n",
        "    predicted=np.argmax(predicted_probs,axis=-1)\n",
        "    output_word=\"\"\n",
        "    for word,index in tn.word_index.items():\n",
        "        if index==predicted:\n",
        "            output_word=word\n",
        "            break\n",
        "    seed_text+=\" \"+output_word\n",
        "print(seed_text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j80ncDLGQ1bE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}